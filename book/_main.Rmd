---
title: "Applied Data Skills"
subtitle: "Processing & Presenting Data"
author: "Emily Nordmann and Lisa DeBruine"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
csl: include/apa.csl
link-citations: yes
description: |
  This book provides an overview of the basic skills needed to turn raw data into informative summaries and visualisations presented in professional reports and presentations. The book will introduce learners to R, a programming language that can help automate working with data. The book will cover importing and processing data from spreadsheets, producing data summaries of descriptive statistics in tables, creating beautiful and informative visualisations, and constructing reports and presentations that automatically update when the underlying data changes.
url: https://psyteachr.github.io/ads-v1
github-repo: psyteachr/ads-v1
cover-image: images/logos/logo
apple-touch-icon: images/logos/apple-touch-icon.png
apple-touch-icon-size: 180
favicon: images/logos/favicon.ico
---

# Overview {.unnumbered}

Placeholder


## Structure of the course
## How to learn data skills

<!--chapter:end:index.Rmd-->


# Intro to R and RStudio {#intro}

Placeholder


## Intended Learning Outcomes {#ilo-intro}
## Walkthrough video
## R and RStudio {#intro-r-rstudio}
## Installing R and RStudio {#intro-installing-r}
### RStudio {#rstudio_ide}
### Reproducibility {#intro-reproducibility}
### Themes and accessiblilty
## Sessions {#intro-sessions}
## Packages and functions {#packages}
### Installing a package {#install-package}
### Loading a package
### Using a function
### Tidyverse
### Function Help
### Arguments
### Argument names
### Tab auto-complete
## Objects
## Getting help {#help}
### Package reference manuals
### Googling
### Vignettes
## Glossary {#glossary-intro}
## Further Resources {#resources-intro}

<!--chapter:end:01-intro.Rmd-->


# Reports with R Markdown {#reports}

Placeholder


## Intended Learning Outcomes {#ilo-reports}
## Organising a project {#projects}
### Start a Project {#project-start}
### Naming Things {#naming}
## R Markdown {#rmarkdown}
### New document
### Code chunks {#code-chunks}
### Running code
### Inline code {#rmd-inline-r }
### Knitting your file {#rmd-knit}
## Loading data
### Online sources {#loading-online}
### Local data files
## Writing a report
### Data analysis
### Text formatting {#markdown}
### Code comments {#comments}
### Images {#rmd-images}
### Tables {#rmd-tables}
## Refining your report
### Chunk defaults {#rmd-setup}
### Override defaults
### Loading packages
### YAML header {#yaml}
### Summary
## Exercises {#exercises-reports}
### New project {#exercises-reports-project}
### New script {#exercises-reports-setup}
### R Markdown {#exercises-reports-rmarkdown}
### Tables {#exercises-reports-tables}
### Images {#exercises-reports-images}
### Inline R {#exercises-reports-inline}
### Knit {#exercises-reports-knit}
## Glossary {#glossary-reports}
## Further Resources {#resources-reports}

<!--chapter:end:02-reports.Rmd-->


# Basic Data Visualisation {#viz}

Placeholder


## Intended Learning Outcomes {#ilo-viz}
## Set-up
## Variable types
### Continuous
### Categorical
### Dates and times
### Test your understanding
## Building plots
### Plot Data {#plots-loading-data}
### Plot setup
#### Default theme
#### Data {#plot-setup-data}
#### Mapping
#### Geoms
#### Multiple geoms
#### Saving plots
#### Combining plots
### Customising plots
#### Styling geoms
#### Format axes
#### Axis limits
#### Themes
#### Theme tweaks
## Appropriate plots
### Counting categories
#### Bar plot
#### Column plot
#### Pie chart
#### Test your understanding
### One continuous variable
#### Histogram
#### Frequency plot
#### Density plot
#### Test your understanding
### Grouped continuous variables
#### Subdividing distributions
#### Comparing distributions
#### Violin plot
#### Boxplot
#### Combo plots
#### Test your understanding
### Two continuous variables
#### Scatterplot
#### Trendlines
#### Dates
### Ordinal variables
#### Jitter plot
#### Facets
## Exercises
### New Markdown {#exercises-new-rmd-3}
### Summary
### Presenting plots
### Combining plots
### Editing your Markdown display
### Change the output
### Share your work
## Glossary {#glossary-viz}
## Further Resources {#resources-viz}

<!--chapter:end:03-viz.Rmd-->


# Data Import {#data}

Placeholder


## Intended Learning Outcomes {#ilo-data}
## Set-up
## Built-in data {#builtin}
## Looking at data
### View() 
### print() 
### glimpse() 
### summary() {#summary-function}
## Importing data {#import_data}
### rio::import()  
### File type specific import 
### Column data types {#col_types}
## Creating data 
## Writing data
## Troubleshooting
## Working with real data
## Exercises
### New Markdown {#exercises-new-rmd-4}
### Import and export the dataset {#exercises-load}
### Convert column types
### Plots {#exercises-plots}
### Make it look nice
### Share your work
## Glossary {#glossary-data}
## Further resources {#resources-data}

<!--chapter:end:04-data.Rmd-->

# Data Summaries {#summary}

## Intended Learning Outcomes {#ilo-summary}

* Be able to summarise data by groups
* Be able to produce well-formatted tables

In this chapter we'll use the following packages:

```{r setup-summary, message=FALSE}
library(tidyverse)   # data wrangling functions
library(rtweet) # for searching tweets
library(kableExtra)  # for nice tables
library(glue) # for pasting strings
```

## Set-up

First, create a new project for the work we'll do in this chapter named `r path("05-summary")`. Second, download the data for this chapter <a href="data/ncod_tweets.rds" download>ncod_tweets.rds</a> and save it in your project data folder. Finally, open and save and new R Markdown document named `summary.Rmd`, delete the welcome text and load the required packages for this chapter.

## Social media data

In this chapter we're going to analyse social media data, specifically data from Twitter. There are two broad types of data you can obtain from Twitter; data scraped from Twitter using purpose-built packages such as `r pkg("rtweet")`, and data provided via [Twitter Analytics](https://analytics.twitter.com/) for any accounts for which you have access. 

For this chapter, we'll use data scraped from Twitter using `r pkg("rtweet")`. In order to use these functions, you need to have a Twitter account although don't worry if you don't have one, we'll also provide the data for you.

`r pkg("rtweet")` has a lot of flexibility, for example, you can search for tweets that contain a certain hashtag or word, tweets by a specific user, or tweets that meet certain conditions like location or whether the user is verified. 

For the dataset for this chapter, we used the `search_tweets()` function  to find the last 30K tweets with the hashtag [#NationalComingOutDay](https://en.wikipedia.org/wiki/National_Coming_Out_Day).This is mainly interesting around October 11th (the date of National Coming Out Day), so we've provided the relevant data for you that we scraped at that time. 

If you have a Twitter account, you can complete this chapter using your own data and any hashtag that interests you. When you run the `search_tweets()` function you will be asked to sign in to your Twitter account.

```{r, eval = FALSE}
tweets <- search_tweets(q = "#NationalComingOutDay", 
                        n = 30000, 
                        include_rts = FALSE)
```

### R objects

If you're working with live social media data, every time you run a query it's highly likely you will get a different set of data as new tweets are added. Additionally, the Twitter API places limits on how much data you can download and searches are limited to data from the last 6-9 days. Consequently, it can be useful to save the results of your initial search. `saveRDS` is a useful function that allows you to save any object in your environment to disk

```{r eval = FALSE}
saveRDS(tweets, file = "data/ncod_tweets.rds")
```

To load an `.rds` file, you can use the `readRDS` function. If you don't have access to a Twitter account, or to ensure that you get the same output as the rest of this chapter, you can now load in `ncod_tweets.rds` using this function.

```{r}
tweets <- readRDS("data/ncod_tweets.rds")
```

## Data summaries

### Summarise

First, run `glimpse(tweets)` and click on the object in the environment to find out what information is in the downloaded data (it's a lot!). Now let's create a series of summary tables and plots with these data.

The `summarise()` function from the `r pkg("dplyr")` package is loaded as part of the tidyverse and creates summary statistics. Check the [Data Transformation Cheat Sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf) for various summary functions. Some common ones are: `n()`, `min()`, `max()`, `sum()`, `mean()`, and `quantile()`.

This function can be used to answer questions like: How many tweets were there? What date range is represented in these data? What are the mean and median number of favourites per tweet? Let's start with a very simple example to calculate the mean, median, min, and max number of favourites (Twitter's version of a "like"):

* The first argument `summarise()` takes is the data table you wish to summarise, in this case the object `tweets`.
* `summarise()` will create a new table. The column names of this new table will be the left hand-side arguments, i.e., `mean_favs` and `median_favs`. 
* The values of these columns are the result of the summary operation on the right hand-side.

```{r}
favourite_summary <- summarise(tweets,
                           mean_favs = mean(favorite_count),
                           median_favs = median(favorite_count),
                           min_favs = min(favorite_count),
                           max_favs = max(favorite_count))
```

```{r echo = FALSE}
favourite_summary %>%
  kable(align = "c")
```

The mean number of favourites is substantially higher than the median and the range is huge, suggesting there are outliers. A quick histogram confirms this - most tweets have zero favourites but there are a few  with a lot of likes (so few you can't really see them on the histogram) that skew the mean.

```{r warning = FALSE, message = FALSE}
ggplot(tweets, aes(x = favorite_count)) +
  geom_histogram()
```

You can add multiple operations to a single call to `summarise()` so let's try a few different operations. The `n()` function counts the number of rows in the data. The `created_at` column gives us the date each tweet were created, so we can use the `min()` and `max()` functions to get the range of dates. 

```{r}
tweet_summary <- tweets %>%
  summarise(mean_favs = mean(favorite_count),
            median_favs = quantile(favorite_count, .5),
            n = n(),
            min_date = min(created_at),
            max_date = max(created_at))

glimpse(tweet_summary)
```

::: {.info data-latex=""}
`r glossary("quantile", "Quantiles")` are like percentiles. Use `quantile(x, .50)` to find the median (the number where 50% of values in `x` are above it and 50% are below it). This can be useful when you need a value like "90% of tweets get *X* favourites or fewer".

```{r}
quantile(tweets$favorite_count, 0.90)
```

:::

```{r, echo=FALSE}
mcq1 <- c(answer = "`tweets %>% summarise(max_retweets = max(retweets))`",
          x = "`tweets %>% summarise(max = retweets)`",
          x = "`tweets %>% summarise(max_retweets)`", 
          x = "`tweets %>% max(retweets)`") %>%
  sample() %>% longmcq()

mcq2 <- c(answer = "`summarise(width = mean(display_text_width))`",
          x = "`width(mean(display_text_width))`",
          x = "`summarise(display_text_width = mean)`",
          x = "`group_by(display_text_width)`") %>%
  sample() %>% longmcq()
```


::: {.try data-latex=""}
* How would you find the largest number of retweets?
    `r mcq1`
* How would you calculate the mean `display_text_width`? 
    `r mcq2`
:::

### The $ operator

We need to take a couple of brief detours to introduce some additional coding conventions. First, let's clear up what that `$` notation is doing. The dollar sign allows you to select variables from an object. The left-hand side is the object, and the right-hand side is the variable. When you call a variable like this, R will print all the observations in that variable. 

```{r}
tweet_summary$mean_favs
```

If your variable has multiple observations, you can specify which ones to return using square brackets `[]` and the row number.

```{r}
tweets$source[1] # select one observation
tweets$display_text_width[c(20,30,40)] # select multiple with c()
```

### Pipes

For our second detour, let's formally introduce the pipe, that weird `%>%` symbol we've used occasionally. Pipes allow you to send the output from one function straight into another function. Specifically, they send the result of the function before `%>%` to be the first argument of the function after `%>%`. It can be useful to translate the pipe as **and then**. It's easier to show than tell so let's look at an example.

We could write the above code using a pipe as follows:

```{r eval = FALSE}
tweet_summary <- tweets %>% # start with the object tweets and then
  summarise(mean_favs = mean(favorite_count), #summarise it
            median_favs = median(favorite_count))
```

Notice that `summarise()` no longer needs the first argument to be the data table, it is pulled in from the pipe. The power of the pipe may not be obvious now, but it will soon prove its worth. 
### Inline coding

To insert those values into the text of a report you can use inline coding. First. we'll create another set of objects that contain the first and last date of the tweets in our dataset. `format()` formats the dates to day/month/year.

```{r}
date_from <- tweet_summary$min_date %>% 
  format("%d %B, %Y")
date_to <- tweet_summary$max_date %>% 
  format("%d %B, %Y")
```

Then you can insert values from these objects and the tables you created with `summarise()` using inline R (note the dollar sign notation). Knit your Markdown to see the output. 

```{r eval = FALSE}
There were `r backtick("r tweet_summary$n")` tweets between `r backtick("r date_from")` and `r backtick("r date_to")`.
```
There were `r tweet_summary$n` tweets between `r date_from` and `r date_to`.

Ok, let's get back on track.

### Counting

How many different accounts tweeted using the hashtag? Who tweeted most?

You can count categorical data with the `count()` function. Since each row is a tweet, you can count the number of rows per each different `screen_name` to get the number of tweets per user. This will give you a new table with each combination of the counted columns and a column called `n` containing the number of observations from that group. 

`sort = TRUE` will sort the table by `n` in descending order whilst `head()` returns the first six lines of a data table and is a useful function to call when you have a very large dataset and just want to see the top values.

```{r}
tweets_per_user <- tweets %>%
  count(screen_name, sort = TRUE)

head(tweets_per_user)
```

```{r, echo = FALSE}
mcq1 <- c(answer = "`tweets %>% count(is_quote, is_retweet)`", 
          x = "`tweets %>% count(is_quote) %>% count(is_retweet)`", 
          x = "`tweets %>% count(c(is_quote, is_retweet))`", 
          x = "`tweets %>% select(is_quote, is_retweet) %>% count()`") %>%
  longmcq()
```

::: {.try data-latex=""}
* How would you count the number of tweets that are quotes or not and are retweets or not? 
    `r mcq1`
:::

### Inline coding 2

Let's do another example of inline coding that writes up a summary of the most prolific tweets to demonstrate a few additional functions. First, we need to create some additional objects to use with inline R:

* `nrow` simply counts the number of rows in a dataset so if you have one user/participant/customer per row, this is an easy way to do a head count.
* `slice()` selects a particular row of data, in this case the first row. Because we sorted our data, this will therefore be the user with the most tweets.
* `pull()` pulls out a single variable. 
* The combination of `slice()` and `pull()` allows you to select a single observation from a single variable.

```{r}
unique_users <- nrow(tweets_per_user)
most_prolific <- slice(tweets_per_user, 1) %>% 
  pull(screen_name)
most_prolific_n <- slice(tweets_per_user, 1) %>% 
  pull(n)
```

Then add the inline code to your report and knit your Markdown to see the output:

```{r eval = FALSE}
There were `r unique_users` unique accounts tweeting about #NationalComingOutDay. `r `most_prolific` was the most prolific tweeter, with `r most_prolific_n` tweets.
```

There were `r unique_users` unique accounts tweeting about #NationalComingOutDay. `r most_prolific` was the most prolific tweeter, with `r most_prolific_n` tweets.

### Grouping

You can also create other summary values by group. The combination of `group_by()` and `summarise()` is incredibly powerful, and it is also a good demonstration of why pipes are so useful.

`group_by()` takes an existing data table and converts it into a grouped table where any operations that are performed on it are doing "by group".

The first line of code creates an object named `tweets_grouped`, that groups the dataset according to whether the user is a verified user. On the surface, `tweets_grouped` doesn't look any different to the original `tweets`, however, the underlying structure has changed and so when we run `summarise()`, we now get our requested summaries for each group (in this case verified or not). 

```{r}
tweets_grouped <- tweets %>%
  group_by(verified)

verified <- tweets_grouped %>%
  summarise(count = n(),
            mean_favs = mean(favorite_count),
            mean_retweets = mean(retweet_count)) %>%
  ungroup()

verified
```

::: {.warning data-latex=""}
Make sure you call the `ungroup()` function when you are done with grouped functions. Failing to do this can cause all sorts of mysterious problems if you use that data table later assuming it isn't grouped.
:::

Whilst the above code is functional, it adds an unnecessary object to the environment - `tweets_grouped` is taking up space and increases the risk we'll use this grouped object by mistake. Enter the pipe.

Rather than creating an intermediate object, we can use the pipe to string our code together.

```{r}
verified <- tweets_grouped %>% # Start with the original dataset and then;
  group_by(verified) %>% # group it and then
  summarise(count = n(), # summarise it by those groups
            mean_favs = mean(favorite_count),
            mean_retweets = mean(retweet_count)) %>%
  ungroup()
```

```{r, echo = FALSE}
mcq3 <- c(answer = "`group_by(screen_name)`",
          x = "`summarise(screen_name)`",
          x = "`count(screen_name)`",
          x = "`mean(screen_name)`") %>%
  sample() %>% longmcq()
```

::: {.try data-latex=""}
* What would you change to calculate the mean favourites and retweets by `screen_name` instead of by `verified`? 
    `r mcq3`
:::

### Multiple groupings

You can add multiple variables to `group_by()` to further break down your data. For example, the below gives us the number of likes and retweets broken down by verified status and the device the person was tweeting from. 

* Reverse the order of `verified` and `source` in `group_by()` to see how it changed the output.

```{r}
verified_source <- tweets %>%
  group_by(verified, source) %>%
  summarise(count = n(),
            mean_favs = mean(favorite_count),
            mean_retweets = mean(retweet_count)) %>%
  ungroup() %>%
  arrange(desc(count))

head(verified_source)
```

::: {.warning data-latex=""}
If you get the following message when using `group_by()`, please update tidyverse.

> `summarise()` ungrouping output (override with `.groups` argument)
:::

### Filter and mutate

You can also use additional functions like `filter()` or `mutate()` after `group_by`. You'll learn more about these in Chapter \ \@ref(wrangle) but briefly:

* `filter()` selects observations (rows) according to specified criteria, e.g., all values above 5, all verified users.
* `mutate()` creates new variables (columns), or overwrites existing ones.

You can combine functions like this to get detailed insights into your data. For example, what was the most favourited original and quoted tweet? 

* The variable `is_quote` tells us whether the tweet in question was an original tweet or a quote tweet. Because we want our output to treat these separately, we pass this variable to `group_by()`. 
* We want the most favourited tweets, i.e., the maximum value of `favourite_count`, so we can use `filter()` to only return rows where `favourite_count` is equal to the maximum value in the variable `favourite_count`. Note the use of `==` rather than a single `=`.
* Just in case there was a tie, choose a random one with `sample_n(size = 1)`.

```{r}
most_fav <- tweets %>%
  group_by(is_quote) %>%
  filter(favorite_count == max(favorite_count)) %>%
  sample_n(size = 1) %>%
  ungroup()
```

### Inline coding 3

There's a huge amount of data reported for each tweet, including things like the URLs of the tweets and any media attached to  them. This means we can produce output like the below reproducibly and using inline coding.

```{r echo = FALSE}
orig <- filter(most_fav, !is_quote)
quote <- filter(most_fav, is_quote)
```

> The most favourited `r orig$favorite_count` original tweet was by [`r orig$screen_name`](`r `orig$status_url`):

--------------------------------------------------
> `r orig$text`

![](orig$ext_media_url)

------------------------------------------------

To produce this, first we split `most_fav`, so that we have one object that contains the data from the original tweet and one object that contains the data from the quote tweet.

```{r echo = FALSE}
orig <- filter(most_fav,is_quote == FALSE)
quote <- filter(most_fav,is_quote == TRUE)
```

The inline code is then as follows:

```{r eval = FALSE}
> The most favourited `r orig$favorite_count` original tweet was by [`r orig$screen_name`](`r `orig$status_url`):

--------------------------------------------------
> `r orig$text`

![](orig$ext_media_url)
--------------------------------------------------
```

This is quite complicated so let's break it down. 

* The `>` symbol changes the format to a block quote.
* The first bit of inline coding is fairly standard and is what you have used before.
* The second bit of inline coding inserts a URL. The content of the `[]` is the text that will be displayed. The content of `()` is the underlying URL. In both cases the content is being pulled from the dataset. In this case, the text is `screen_name` and `status_url` links to the tweet.
* The line of dashes creates the solid line in the knitted output.
* The image is then included using the format `![]()` which is alternative method of including images in Markdown. 


```{r, echo = FALSE}
mcq1 <- c(answer = "`tweets %>% group_by(source) %>% filter(n() >= 10)`", 
          x = "`tweets %>% group_by(source) %>% select(n() >= 10)`", 
          x = "`tweets %>% group_by(source) %>% filter(count() >= 10)`", 
          x = "`tweets %>% group_by(source) %>% select(count() >= 10)`") %>%
  sample() %>% longmcq()
```

::: {.try data-latex=""}
* How would you limit the results to sources with 10 or more rows?
    `r mcq1`
:::

## Putting it together {#together-summary}

For the final step in this chapter, we will create a table of the top five hashtags used in conjunction with #NationalComingOutDay, the total number of tweets in each hashtag, the total number of likes, and the top tweet for each hashtag.

The function `select()` is useful for just keeping the variables (columns) you need to work with, which can make working with very large datasets easier. The arguments to `select()` are simply the names of the variables and the resulting table will present them in the order you specify.  

```{r}
tweets_with_hashtags <- tweets %>%
  select(hashtags, text, favorite_count, media_url)
```

Look at the dataset using `View(tweets_with_hashtags)`. You'll notice that the variable `hashtags` has multiple values in each cell (i.e., when users used more than one hashtag in a single tweet). In order to work with this information, we need to separate each hashtag so that each row of data represents a single hashtag. We can do this using the function `unnest()` and adding a pipeline of code.

```{r}
tweets_with_hashtags <- tweets %>%
  select(hashtags, text, favorite_count, media_url)%>%
  unnest(cols = hashtags)
```

To get the top 5 hashtags we need to know how tweets used each one This code uses pipes to build up the analysis. When you encounter multi-pipe code, it can be very useful to run each line of the pipeline to see how it builds up and to check the output at each step. This code:

* Starts with the object tweets and then;
* Counts the number of tweets for each hashtag using `count()` and then;
* Filters out any blank cells using `!is.na()` (you can read this as "keep any value that isn't a missing value) and then;
* Returns the top five values using `slice_max()` and orders them by the `n` column.

```{r}
top5_hashtags <- tweets_with_hashtags %>%
  count(hashtags) %>%
  filter(!is.na(hashtags)) %>%  # get rid of the blank value
  slice_max(order_by = n, n = 5)

top5_hashtags
```

Two of the hashtags are the same, but with different case. We can fix this by adding in an extra line of code that uses `mutate()` to overwrite the variable `hashtag` with the same data but transformed to lower case.

```{r}
top5_hashtags <- tweets_with_hashtags %>%
  mutate(hashtags = tolower(hashtags)) %>%
  count(hashtags) %>%
  filter(!is.na(hashtags)) %>%  # get rid of the blank value
  slice_max(order_by = n, n = 5)

top5_hashtags
```

Next, get the top tweet for each hashtag using `filter()`. Use `group_by()` before you filter to select the most-liked tweet in each hashtag, rather than the one with most likes overall. As you're getting used to writing and running this kind of multi-step code, it can be very useful to take out individual lines and see how it changes the output to strengthen your understanding of what each step is doing.

```{r}
top_tweet_per_hashtag <- tweets_with_hashtags %>%
  mutate(hashtags = tolower(hashtags)) %>%
  group_by(hashtags) %>%
  filter(favorite_count == max(favorite_count)) %>%
  sample_n(size = 1) %>%
  ungroup()
```

Get the total number of likes per hashtag by grouping and summarising with `sum()`.

```{r}
likes_per_hashtag <- tweets_with_hashtags %>%
  mutate(hashtags = tolower(hashtags)) %>%
  group_by(hashtags) %>%
  summarise(total_likes = sum(favorite_count)) %>%
  ungroup()
```

We can put everything together using `left_join()`. This will keep everything from the first table specified and then add on the relevant data from the second table specified. In this case, we add on the data in `top_tweet_per_hashtag` and `like_per_hashtag` but only for the tweets included in `top5_hashtags`

```{r}
top5 <- top5_hashtags %>%
  left_join(top_tweet_per_hashtag, by = "hashtags") %>%
  left_join(likes_per_hashtag, by = "hashtags") 
```

Before we can finish up though, there's a couple of extra steps we need to add in to account for some of the idiosyncrasies of Twitter data. 

First,  the `@` symbol is used by R Markdown for referencing (we'll show you how to do this in Chapter \ \@ref(present)). It's likely that some of the tweets will contain this symbol, so we can use mutate to find any instances of `@` and add backslashes. Backslashes create a literal version of characters that have a special meaning in R, so adding them means it will print the `@` symbol without any issues. Of course `\` also has a special meaning in R which means we also need to backslash the backslash. Isn't programming fun? We can use the same code to tidy up any ampersands (&) which sometimes display as "&amp".

Second, if there are multiple images associated with a single tweet, `media_url` will be a list, so we use `unlist()`.

Finally, we use `select()` to tidy up the table and just keep the columns we need.

```{r}
top5 <- top5_hashtags %>%
  left_join(top_tweet_per_hashtag, by = "hashtags") %>%
  left_join(likes_per_hashtag, by = "hashtags") %>%
  # replace @ with \@ so @ doesn't trigger referencing
  mutate(text = gsub("@", "\\\\@", text),
         text = gsub("&amp;", "&", text)) %>%
  # media_url can be a list if there is more than one image
  mutate(image = unlist(media_url)) %>%
  # put the columns you want to display in order
  select(hashtags, n, total_likes, text, image)

top5
```

Whilst this table now has all the information we want, it isn't great aesthetically. The `r pkg("kableExtra")` package has functions that will improve the presentation of tables. We're going to show you two examples of how you could format this table. 

The first is (relatively) simple and stays within the R programming language using functionality from `r pkg("kableExtra")`. The main aesthetic feature of the table is the incorporation of the pride flag colours for each row. Each row is set to a different colour of the pride flag and the text is set to be black and bold to improve the contrast. We've also removed the `image` column as it just contains a URL.

```{r}
# the hex codes of the pride flag colours, obtained from https://www.schemecolor.com/lgbt-flag-colors.php

pride_colours <- c("#FF0018", "#FFA52C", "#FFFF41", "#008018", "#0000F9", "#86007D")

top5 %>%
  select(-image) %>%
  kable(col.names = c("Hashtags", "No. tweets", "Likes", "Tweet"),
        caption = "Stats and the top tweet for the top five hashtags.") %>%
  row_spec(row = 0, 
           background = pride_colours[1], color = "black", 
           bold = T, font_size = 18) %>%
  row_spec(row = 1, background = pride_colours[2], color = "black", 
           bold = T)%>%
  row_spec(row = 2, background = pride_colours[3], color = "black", 
           bold = T)%>%
  row_spec(row = 3, background = pride_colours[4], color = "black", 
           bold = T)%>%
  row_spec(row = 4, background = pride_colours[5], color = "black", 
           bold = T)%>%
  row_spec(row = 5, background = pride_colours[6], color = "black", 
           bold = T)
```

An alternative approach incorporates `r glossary("HTML")` and also uses the package `r pkg("glue")` to combine information from multiple columns. 

First, we use `mutate()` to create a new column `col1` that combines the first three columns into a single column and adds some formatting to make the hashtag bold (`<strong>`) and insert line breaks (`<br>`). We'll also change the image column to display the image using html if there is an image.

If you're not familiar with HTML, don't worry too if you don't understand the below code, the point is to show you the full extent of the flexibility available.

```{r eval = FALSE}
top5 %>%
  mutate(col1 = glue("<strong>#{hashtags}</strong>
                     <br>
                     tweets: {n}
                     <br>
                     likes: {total_likes}"),
         img = ifelse(!is.na(image),
                      glue("<img src='{image}' width='200px' />"),
                      "")) %>%
  select(col1, text, img) %>%
  kable(
    escape = FALSE, # allows HTML in the table
    col.names = c("Hashtag", "Top Tweet", ""),
    caption = "Stats and the top tweet for the top five hashtags.") %>%
  column_spec(1:2, extra_css = "vertical-align: top;") %>%
  row_spec(0, extra_css = "vertical-align: bottom;") %>%
  kable_paper()
```

## Exercises

That was an intensive chapter! Take a break and then try one (or more) of the following and post your knitted HTML files on Teams so that other learners on the course can see what you did.

* If you have your own Twitter account, conduct a similar analysis of a different hashtag.
* Look through the rest of the variables in `tweets`, what other insights can you generate about this data?
* Read through the [kableExtra](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html) vignettes and apply your own preferred table style.
* Work through the first few chapters of [Tidy Text](https://www.tidytextmining.com/index.html) to see how you can work with and analyse text. In particular, see if you can conduct a sentiment analysis on the tweet data.

## Glossary {#glossary-summary}

`r glossary_table()`

## Further resources {#resources-summary}

* [Data transformation cheat sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf)
* [Chapter 5: Data Transformation ](http://r4ds.had.co.nz/transform.html) in *R for Data Science*
* [Intro to rtweet](https://docs.ropensci.org/rtweet/articles/rtweet.html)
* [Tidy Text](https://www.tidytextmining.com/index.html)
* [kableExtra vignettes](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html)

<!--chapter:end:05-summary.Rmd-->


# Data Relations {#joins}

Placeholder


## Intended Learning Outcomes {#ilo-joins}
## Mutating Joins
### left_join() {#left_join}
### right_join() {#right_join}
### inner_join() {#inner_join}
### full_join() {#full_join}
## Filtering Joins
### semi_join() {#semi_join}
### anti_join() {#anti_join}
## Binding Joins
### bind_rows() {#bind_rows}
### bind_cols() {#bind_cols}
### Importing folders
## Set Operations
### intersect() {#intersect}
### union() {#union}
### setdiff() {#setdiff}
## Glossary {#glossary-joins}
## Further resources {#resources-joins}

<!--chapter:end:06-joins.Rmd-->


# Data Tidying {#tidy}

Placeholder


## Intended Learning Outcomes {#ilo-tidy}
## Data Structure
### Untidy data
### Tidy data
## Reshaping Data
#### Wide to long
#### Long to wide
## Multi-step tidying {#multistep}
### One observation per row
### One variable per column
### One value per cell
### Altering data
### Fixing data types
## Pipes {#pipes}
### Multi-step pipes
## Glossary {#glossary-tidy}
## Further resources {#resources-tidy}

<!--chapter:end:07-tidy.Rmd-->


# Data Wrangling {#wrangle}

Placeholder


## Intended Learning Outcomes {#ilo-wrangle}
## Wrangling functions
### Select
### Filter
### Arrange
### Mutate
## Putting it together {#together-wrangle}
## Glossary {#glossary-wrangle}
## Further resources {#resources-wrangle}

<!--chapter:end:08-wrangle.Rmd-->


# Customising Visualisations {#custom}

Placeholder


## Intended Learning Outcomes {#ilo-custom}
## Defaults
## R Markdown Options
### Image Size
### Output 
### Captions
### Setup Chunk
## Themes and Colours
## Other Plots
### Interactive Plots
### Waffle Plots
### Lollipop Plots
### Treemap
### Word Clouds
### Maps
## Further Resources {#resources-custom}

<!--chapter:end:09-custom.Rmd-->


# Advanced Reports  {#present}

Placeholder


## Intended Learning Outcomes {#ilo-present}
## Reports
### Word documents
### PDF documents
### Linked documents
### References
#### Creating a BibTeX file
#### Adding references
#### Citing references
#### Citation styles
#### Reference section
### Interactive tables
## Other formats
### Presentations
### Dashboards
### Books
### Websites
### Shiny
## Further resources {#resources-present}

<!--chapter:end:10-present.Rmd-->

# (APPENDIX) Appendices {-} 

<!--chapter:end:appendix-0.Rmd-->


# Installing `R` {#installing-r}

Placeholder


## Installing Base R
## Installing RStudio
## RStudio Settings {#rstudio-settings}
## Installing LaTeX

<!--chapter:end:appendix-a-installing-r.Rmd-->


# Updating R, RStudio, and packages {#appendix-updating-r}

Placeholder


## Updating RStudio
## Updating packages
## Updating R

<!--chapter:end:appendix-b-updating-r.Rmd-->


# Symbols {#symbols}

Placeholder



<!--chapter:end:appendix-c-symbols.Rmd-->


# Conventions

Placeholder



<!--chapter:end:appendix-d-conventions.Rmd-->


# Data Types {#data-types}

Placeholder


## Basic data types 
### Numeric data
### Character data
### Logical Data
### Factors
### Dates and Times
## Basic container types {#containers}
### Vectors {#vectors}
#### Selecting values from a vector
#### Repeating Sequences {#rep_seq}
### Lists
### Tables {#tables-data}
#### Table info
#### Accessing rows and columns {#row-col-access}
## Glossary {#glossary-datatypes}

<!--chapter:end:appendix-e-datatypes.Rmd-->


# Styling Plots {#plotstyle}

Placeholder


## Aesthetics
### Colour/Fill
### Alpha
### Shape
### Linetype
## Palettes
### Viridis Palettes
#### Discrete Viridis Palettes
#### Continuous Viridis Palettes
### Brewer Palettes
#### Qualitative Brewer Palettes
#### Sequential Brewer Palettes
#### Diverging Brewer Palettes
## Themes {#themes-appendix}
### ggthemes
### Fonts

<!--chapter:end:appendix-f-styling.Rmd-->


# Twitter Example

Placeholder


## Single Data File
### Export Data
### Import Data
### Select Relevant Data
### Plot Likes per Day
### Plot Multiple Engagements
## Multiple Data Files
### Likes by Month
## Data by Tweet
### Impressions per Tweet
### Top Tweet
### Word Cloud
### Tweets by Hour

<!--chapter:end:appendix-g-twitter.Rmd-->


# Webpages

Placeholder


## Create a webpage {#webpage-create}
### Create a project 
### Site header
### Edit the pages
### Render the site
## Add pages {#webpage-pages}
## Styles {#webpage-styles}
### Add custom styles
### Change global fonts and colours
### Change certain elements
## Example using the styles above
### Level 3 header
#### Level 4 header

<!--chapter:end:appendix-h-webpage.Rmd-->

# License {-}

This book is licensed under Creative Commons Attribution-ShareAlike 4.0 International License [(CC-BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/){target="_blank"}. You are free to share and adapt this book. You must give appropriate credit, provide a link to the license, and indicate if changes were made. If you adapt the material, you must distribute your contributions under the same license as the original. 

<!--chapter:end:appendix-y-license.Rmd-->

`r if (knitr::is_html_output()) '# References {-}'`


<!--chapter:end:appendix-z-refs.Rmd-->

