[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"book provides overview basic skills needed turn raw data informative summaries visualisations presented professional reports presentations. book introduce learners R (R Core Team, 2021), programming language can help automate working data. book cover importing processing data spreadsheets, producing data summaries descriptive statistics tables, creating beautiful informative visualisations, constructing reports presentations automatically update underlying data changes.end book, able use R :clean process datasummarise datainformatively visualise datacreate reusable report templates","code":""},{"path":"index.html","id":"structure-of-the-course","chapter":"Overview","heading":"0.1 Structure of the course","text":"book accompanies 10-week course, covering one chapter per week. chapter introduce new skills concepts using concrete examples. various points, multiple-choice fill---blank questions check understanding. chapter accompanying walk-videos, instructor demonstrates skills covered chapter. chapter also accompanying exercises reinforce learning.","code":""},{"path":"index.html","id":"how-to-learn-data-skills","chapter":"Overview","heading":"0.2 How to learn data skills","text":"Learning data skills kind like gym membership (thanks Phil McAleer analogy). given state---art equipment use instructions use , data skills get stronger unless practice.Data skills require memorise lots code. introduced many different functions, main skill learn efficiently find information need. require getting used structure help files cheat sheets, learning Goggle problem choose helpful solution, learning read error messages.Learning code involves making lot mistakes. mistakes completely essential process, try feel frustrated. Many chapter exercises give broken code fix get experience seeing common errors look like. become experienced coder, might make fewer errors, recover much faster.","code":""},{"path":"summary.html","id":"summary","chapter":"1 Data Summaries","heading":"1 Data Summaries","text":"","code":""},{"path":"summary.html","id":"ilo-summary","chapter":"1 Data Summaries","heading":"1.1 Intended Learning Outcomes","text":"able summarise data groupsBe able produce well-formatted tablesIn chapter use following packages:","code":"\nlibrary(tidyverse)   # data wrangling functions\nlibrary(rtweet) # for searching tweets\nlibrary(kableExtra)  # for nice tables"},{"path":"summary.html","id":"set-up","chapter":"1 Data Summaries","heading":"1.2 Set-up","text":"First, create new project work chapter named 05-summary. Second, download data chapter ncod_tweets.rds save project data folder. Finally, open save new R Markdown document named summary.Rmd, delete welcome text load required packages chapter.","code":""},{"path":"summary.html","id":"social-media-data","chapter":"1 Data Summaries","heading":"1.3 Social media data","text":"chapter going analyse social media data, specifically data Twitter. two broad types data can obtain Twitter; data scraped Twitter using purpose-built packages rtweet, data provided via Twitter Analytics accounts access.chapter, use data scraped Twitter using rtweet. order use functions, need Twitter account although worry one, also provide data .rtweet lot flexibility, example, can search tweets contain certain hashtag word, tweets specific user, tweets meet certain conditions like location whether user verified.dataset chapter, used search_tweets() function find last 30K tweets hashtag #NationalComingOutDay.mainly interesting around October 11th (date National Coming Day), provided relevant data scraped time.Twitter account, can complete chapter using data hashtag interests . run search_tweets() function asked sign Twitter account.","code":"\ntweets <- search_tweets(q = \"#NationalComingOutDay\", \n                        n = 30000, \n                        include_rts = FALSE)"},{"path":"summary.html","id":"r-objects","chapter":"1 Data Summaries","heading":"1.3.1 R objects","text":"working live social media data, every time run query highly likely get different set data new tweets added. Additionally, Twitter API places limits much data can download searches limited data last 6-9 days. Consequently, can useful save results initial search. saveRDS useful function allows save object environment diskTo load .rds file, can use readRDS() function. access Twitter account, ensure get output rest chapter, can download ncod_tweets.rds load using function.","code":"\nsaveRDS(tweets, file = \"data/ncod_tweets.rds\")\ntweets <- readRDS(\"data/ncod_tweets.rds\")"},{"path":"summary.html","id":"data-summaries","chapter":"1 Data Summaries","heading":"1.4 Data summaries","text":"","code":""},{"path":"summary.html","id":"summarise","chapter":"1 Data Summaries","heading":"1.4.1 Summarise","text":"First, run glimpse(tweets) click object environment find information downloaded data (lot!). Now create series summary tables plots data.summarise() function dplyr package loaded part tidyverse creates summary statistics. Check Data Transformation Cheat Sheet various summary functions. common ones : n(), min(), max(), sum(), mean(), quantile().function can used answer questions like: many tweets ? date range represented data? mean median number favourites per tweet? start simple example calculate mean, median, min, max number favourites (Twitter's version \"like\"):first argument summarise() takes data table wish summarise, case object tweets.summarise() create new table. column names new table left hand-side arguments, .e., mean_favs median_favs.values columns result summary operation right hand-side.mean number favourites substantially higher median range huge, suggesting outliers. quick histogram confirms - tweets zero favourites lot likes (really see histogram) skew mean.can add multiple operations single call summarise() try different operations. n() function counts number rows data. created_at column gives us date tweet created, can use min() max() functions get range dates.Quantiles like percentiles. Use quantile(x, .50) find median (number 50% values x 50% ). can useful need value like \"90% tweets get X favourites fewer\".find largest number retweets?\n\ntweets %>% max(retweets)tweets %>% summarise(max_retweets = max(retweets))tweets %>% summarise(max_retweets)tweets %>% summarise(max = retweets)\ncalculate mean display_text_width?\n\nwidth(mean(display_text_width))summarise(width = mean(display_text_width))summarise(display_text_width = mean)group_by(display_text_width)\n","code":"\nfavourite_summary <- summarise(tweets,\n                           mean_favs = mean(favorite_count),\n                           median_favs = median(favorite_count),\n                           min_favs = min(favorite_count),\n                           max_favs = max(favorite_count))\nggplot(tweets, aes(x = favorite_count)) +\n  geom_histogram()\ntweet_summary <- tweets %>%\n  summarise(mean_favs = mean(favorite_count),\n            median_favs = quantile(favorite_count, .5),\n            n = n(),\n            min_date = min(created_at),\n            max_date = max(created_at))\n\nglimpse(tweet_summary)## Rows: 1\n## Columns: 5\n## $ mean_favs   <dbl> 29.71732\n## $ median_favs <dbl> 3\n## $ n           <int> 28626\n## $ min_date    <dttm> 2021-10-10 00:10:02\n## $ max_date    <dttm> 2021-10-12 20:12:27\nquantile(tweets$favorite_count, 0.90)## 90% \n##  31"},{"path":"summary.html","id":"the-operator","chapter":"1 Data Summaries","heading":"1.4.2 The $ operator","text":"need take couple brief detours introduce additional coding conventions. First, clear $ notation . dollar sign allows select variables object. left-hand side object, right-hand side variable. call variable like , R print observations variable.variable multiple observations, can specify ones return using square brackets [] row number.","code":"\ntweet_summary$mean_favs## [1] 29.71732\ntweets$source[1] # select one observation\ntweets$display_text_width[c(20,30,40)] # select multiple with c()## [1] \"Twitter for Android\"\n## [1]  78 287 107"},{"path":"summary.html","id":"pipes","chapter":"1 Data Summaries","heading":"1.4.3 Pipes","text":"second detour, formally introduce pipe weird %>% symbol used occasionally. Pipes allow send output one function straight another function. Specifically, send result function %>% first argument function %>%. can useful translate pipe \"\". easier show tell, look example.write code using pipe follows:Notice summarise() longer needs first argument data table, pulled pipe. power pipe may obvious now, soon prove worth.","code":"\ntweet_summary <- tweets %>% # start with the object tweets and then\n  summarise(mean_favs = mean(favorite_count), #summarise it\n            median_favs = median(favorite_count))"},{"path":"summary.html","id":"inline-coding","chapter":"1 Data Summaries","heading":"1.4.4 Inline coding","text":"insert values text report can use inline coding. First. create another set objects contain first last date tweets dataset. format() formats dates day/month/year.can insert values objects tables created summarise() using inline R (note dollar sign notation get value n column table tweet_summary).Knit Markdown see variables inside inline code get replaced values.28626 tweets 10 October, 2021 12 October, 2021.Ok, get back track.","code":"\ndate_from <- tweet_summary$min_date %>% \n  format(\"%d %B, %Y\")\ndate_to <- tweet_summary$max_date %>% \n  format(\"%d %B, %Y\")There were `r tweet_summary$n` tweets between `r date_from` and `r date_to`."},{"path":"summary.html","id":"counting","chapter":"1 Data Summaries","heading":"1.4.5 Counting","text":"many different accounts tweeted using hashtag? tweeted ?can count categorical data count() function. Since row tweet, can count number rows per different screen_name get number tweets per user. give new table combination counted columns column called n containing number observations group.argument sort = TRUE sort table n descending order, whilst head() returns first six lines data table useful function call large dataset just want see top values.create table counts ?","code":"\ntweets_per_user <- tweets %>%\n  count(screen_name, sort = TRUE)\n\nhead(tweets_per_user)"},{"path":"summary.html","id":"inline-coding-2","chapter":"1 Data Summaries","heading":"1.4.6 Inline coding 2","text":"another example inline coding writes summary prolific tweets demonstrate additional functions. First, need create additional objects use inline R:nrow simply counts number rows dataset one user/participant/customer per row, easy way head count.slice() chooses particular row data, case first row. sorted data, therefore user tweets.pull() pulls single variable.combination slice() pull() allows choose single observation single variable.add inline code report knit Markdown see output:25189 unique accounts tweeting #NationalComingOutDay. interest_outfit prolific tweeter, 35 tweets.","code":"\nunique_users <- nrow(tweets_per_user)\nmost_prolific <- slice(tweets_per_user, 1) %>% \n  pull(screen_name)\nmost_prolific_n <- slice(tweets_per_user, 1) %>% \n  pull(n)There were `r unique_users` unique accounts tweeting about #NationalComingOutDay. `r most_prolific` was the most prolific tweeter, with `r most_prolific_n` tweets."},{"path":"summary.html","id":"grouping","chapter":"1 Data Summaries","heading":"1.5 Grouping","text":"can also create summary values group. combination group_by() summarise() incredibly powerful, also good demonstration pipes useful.function group_by() takes existing data table converts grouped table, operations performed done \"group\".first line code creates object named tweets_grouped, groups dataset according whether user verified user. surface, tweets_grouped look different original tweets, however, underlying structure changed run summarise(), now get requested summaries group (case verified ).Make sure call ungroup() function done grouped functions. Failing can cause sorts mysterious problems use data table later assuming grouped.Whilst code functional, adds unnecessary object environment - tweets_grouped taking space increases risk use grouped object mistake. Enter... pipe.Rather creating intermediate object, can use pipe string code together.change calculate mean favourites retweets screen_name instead verified?\n\ngroup_by(screen_name)count(screen_name)summarise(screen_name)mean(screen_name)\n","code":"\ntweets_grouped <- tweets %>%\n  group_by(verified)\n\nverified <- tweets_grouped %>%\n  summarise(count = n(),\n            mean_favs = mean(favorite_count),\n            mean_retweets = mean(retweet_count)) %>%\n  ungroup()\n\nverified\nverified <- tweets_grouped %>% # Start with the original dataset and then;\n  group_by(verified) %>% # group it and then\n  summarise(count = n(), # summarise it by those groups\n            mean_favs = mean(favorite_count),\n            mean_retweets = mean(retweet_count)) %>%\n  ungroup()"},{"path":"summary.html","id":"multiple-groupings","chapter":"1 Data Summaries","heading":"1.5.1 Multiple groupings","text":"can add multiple variables group_by() break data. example, gives us number likes retweets broken verified status device person tweeting .Reverse order verified source group_by() see changed output.get following message using group_by(), please update tidyverse.summarise() ungrouping output (override .groups argument)","code":"\nverified_source <- tweets %>%\n  group_by(verified, source) %>%\n  summarise(count = n(),\n            mean_favs = mean(favorite_count),\n            mean_retweets = mean(retweet_count)) %>%\n  ungroup() %>%\n  arrange(desc(count))## `summarise()` has grouped output by 'verified'. You can override using the `.groups` argument.\nhead(verified_source)"},{"path":"summary.html","id":"filter-and-mutate","chapter":"1 Data Summaries","heading":"1.5.2 Filter and mutate","text":"can also use additional functions like filter() mutate() group_by. learn Chapter  ?? briefly:filter() keeps observations (rows) according specified criteria, e.g., values 5, verified users.mutate() creates new variables (columns), overwrites existing ones.can combine functions like get detailed insights data. example, favourited original quoted tweet?variable is_quote tells us whether tweet question original tweet quote tweet. want output treat separately, pass variable group_by().want favourited tweets, .e., maximum value favourite_count, can use filter() return rows favourite_count equal maximum value variable favourite_count. Note use == rather single =.Just case tie, choose random one sample_n(size = 1).","code":"\nmost_fav <- tweets %>%\n  group_by(is_quote) %>%\n  filter(favorite_count == max(favorite_count)) %>%\n  sample_n(size = 1) %>%\n  ungroup()"},{"path":"summary.html","id":"inline-coding-3","chapter":"1 Data Summaries","heading":"1.5.3 Inline coding 3","text":"huge amount data reported tweet, including things like URLs tweets media attached . means can produce output like reproducibly using inline coding.favourited 22935 original tweet jackrooke:produce , first split most_fav, one object contains data original tweet one object contains data quote tweet.inline code follows:quite complicated break .first bit inline coding fairly standard used .second bit inline coding inserts URL. content [] text displayed. content () underlying URL. cases, content pulled dataset. case, text screen_name status_url links tweet.line dashes creates solid line knitted output.> symbol changes format block quote.image included using format ![](url), alternative method including images Markdown.limit results sources 10 rows?\n\ntweets %>% group_by(source) %>% filter(count() >= 10)tweets %>% group_by(source) %>% select(n() >= 10)tweets %>% group_by(source) %>% filter(n() >= 10)tweets %>% group_by(source) %>% select(count() >= 10)\n","code":"\norig <- filter(most_fav,is_quote == FALSE)\nquote <- filter(most_fav,is_quote == TRUE)The most favourited `r orig$favorite_count` original tweet \nwas by [`r orig$screen_name`](`r orig$status_url`):\n\n--------------------------------------------------\n> `r orig$text`\n\n![](orig$ext_media_url)\n--------------------------------------------------"},{"path":"summary.html","id":"exercises","chapter":"1 Data Summaries","heading":"1.6 Exercises","text":"intensive chapter! Take break try one () following post knitted HTML files Teams learners course can see .Twitter account, conduct similar analysis different hashtag.Look rest variables tweets, insights can generate data?Read kableExtra vignettes apply preferred table style.Work first chapters Tidy Text see can work analyse text. particular, see can conduct sentiment analysis tweet data.","code":""},{"path":"summary.html","id":"glossary-summary","chapter":"1 Data Summaries","heading":"1.7 Glossary","text":"","code":""},{"path":"summary.html","id":"resources-summary","chapter":"1 Data Summaries","heading":"1.8 Further resources","text":"Data transformation cheat sheetChapter 5: Data Transformation R Data ScienceIntro rtweetTidy TextkableExtra vignettes","code":""}]
